{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Team Number-8\n",
        "\n",
        "Team Members-Chalani Kalpana,\n",
        "             Nilmini Pusweli,\n",
        "             Thilini Gamage"
      ],
      "metadata": {
        "id": "EPm_et-qEs9Y"
      },
      "id": "EPm_et-qEs9Y"
    },
    {
      "cell_type": "markdown",
      "id": "601f1d82",
      "metadata": {
        "id": "601f1d82"
      },
      "source": [
        "## Food Type Detection Using EfficientNet_B0 on Food101"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XLEN5z4zaNvd",
      "metadata": {
        "id": "XLEN5z4zaNvd"
      },
      "source": [
        "Step 1: Load Pretrained Model-\n",
        "We load the EfficientNet model (e.g., efficientnet_b0) which has been pre-trained on ImageNet. This helps the model start with strong visual features without training from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b117c0a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b117c0a4",
        "outputId": "3173d858-3e1a-4020-8db6-8f62428e5d43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Step 1: Import Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import Food101\n",
        "from torchvision.models import efficientnet_b0\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6E19A-yxaiy1",
      "metadata": {
        "id": "6E19A-yxaiy1"
      },
      "source": [
        "Step 2: Load and Prepare Dataset-\n",
        "We use the Food101 dataset, a large vision dataset of 101 food categories. Images are transformed (resized, normalized, converted to tensor) and split into training and test loaders for training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b81748ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b81748ab",
        "outputId": "b5388e02-079a-4ed6-e327-f229481c19f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.00G/5.00G [03:40<00:00, 22.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Step 2: Load and Transform Food101 Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "food_train = Food101(root='./data', split='train', transform=transform, download=True)\n",
        "food_test = Food101(root='./data', split='test', transform=transform, download=True)\n",
        "\n",
        "# Subsample for faster training/testing\n",
        "train_subset, _ = random_split(food_train, [5000, len(food_train) - 5000])\n",
        "test_subset, _ = random_split(food_test, [1000, len(food_test) - 1000])\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_subset, batch_size=32, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5WeM9QXXaq-I",
      "metadata": {
        "id": "5WeM9QXXaq-I"
      },
      "source": [
        "Step 3: Replace the Classifier\n",
        "Since the pretrained EfficientNet_B0 model was originally trained on ImageNet with 1000 output classes, we need to replace its final classification layer to match our target dataset — Food101 — which has 101 classes. This step ensures that the model can correctly predict the classes relevant to our food recognition task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bf2e076e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf2e076e",
        "outputId": "699f91c4-708c-4f95-93c0-d98691850a8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 101MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Load Pretrained EfficientNet_B0 and Modify Output Layer\n",
        "model = efficientnet_b0(pretrained=True)\n",
        "model.classifier[1] = nn.Linear(model.classifier[1].in_features, 101)\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F-rnYKWlawTx",
      "metadata": {
        "id": "F-rnYKWlawTx"
      },
      "source": [
        "Step 4: Evaluate Pretrained Model (Before Fine-Tuning)\n",
        "We evaluate the model on the test set before any fine-tuning. This gives us a baseline accuracy using the pretrained features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "df6d7e8b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df6d7e8b",
        "outputId": "20b6dd98-6070-4649-981f-1a9f72cad781"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy (before fine-tuning): 0.80%\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Test Accuracy Before Fine-Tuning\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "baseline_acc = evaluate(model, test_loader, device)\n",
        "print(f\"Test Accuracy (before fine-tuning): {baseline_acc:.2f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NgjpRGhia28r",
      "metadata": {
        "id": "NgjpRGhia28r"
      },
      "source": [
        "Step 5: Fine-Tune the Model-\n",
        "We train the model for one epoch on the Food101 training set. This helps the model adapt its learned features to the specific task of food classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e4c45ce1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4c45ce1",
        "outputId": "214934e3-faa9-4fc1-e0ed-1d1b96646bfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy (after fine-tuning): 20.70%\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Fine-Tune for 1 Epoch\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "model.train()\n",
        "for images, labels in train_loader:\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "finetuned_acc = evaluate(model, test_loader, device)\n",
        "print(f\"Test Accuracy (after fine-tuning): {finetuned_acc:.2f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Xxpa02X-a8ep",
      "metadata": {
        "id": "Xxpa02X-a8ep"
      },
      "source": [
        "Step 6: Save Fine-Tuned Model and Measure Model Size\n",
        "After fine-tuning, we save the updated model weights to disk and measure the size of the saved model file. This helps us understand the storage requirements before applying any compression or quantization techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "fdfade53",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdfade53",
        "outputId": "04ceb812-93dd-4cf5-abaa-8e75bb94f6ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size before quantization: 16.85 MB\n"
          ]
        }
      ],
      "source": [
        "# Step 6: Save Model and Measure Size\n",
        "torch.save(model.state_dict(), \"efficientnet_finetuned.pth\")\n",
        "original_size = os.path.getsize(\"efficientnet_finetuned.pth\") / 1e6  # MB\n",
        "print(f\"Model size before quantization: {original_size:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rOmgE2gbbCQG",
      "metadata": {
        "id": "rOmgE2gbbCQG"
      },
      "source": [
        "Step 7: Quantize Model and Measure Size After Quantization\n",
        "We apply dynamic quantization to the fine-tuned model, save the quantized model, and measure its file size. This helps us understand the reduction in model size achieved through quanti"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "SdrK4Sl_QgXp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdrK4Sl_QgXp",
        "outputId": "90d9dca4-4f79-4e24-eabd-c1422ca45513"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size after quantization: 16.46 MB\n"
          ]
        }
      ],
      "source": [
        "# Step 7: Quantize Model\n",
        "quantized_model = torch.quantization.quantize_dynamic(copy.deepcopy(model), {nn.Linear}, dtype=torch.qint8)\n",
        "torch.save(quantized_model.state_dict(), \"efficientnet_quantized.pth\")\n",
        "quantized_size = os.path.getsize(\"efficientnet_quantized.pth\") / 1e6  # MB\n",
        "print(f\"Model size after quantization: {quantized_size:.2f} MB\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VzCr354AbIAB",
      "metadata": {
        "id": "VzCr354AbIAB"
      },
      "source": [
        "Step 8: Apply Dynamic Quantization-\n",
        "We use dynamic quantization to convert Linear layers of the model to 8-bit integers (INT8), which reduces model size and may speed up inference — especially on CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "w6aHZq5FVsde",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6aHZq5FVsde",
        "outputId": "8c0ad680-96c3-4d25-a7fd-dbc4b6426f21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy (after quantizing linear layers only): 21.10%\n"
          ]
        }
      ],
      "source": [
        "# Step 8: Apply Dynamic Quantization\n",
        "from torch.quantization import quantize_dynamic\n",
        "\n",
        "quantized_model = quantize_dynamic(\n",
        "    model.cpu(),                 # move model to CPU before quantization\n",
        "    {torch.nn.Linear},           # specify layers to quantize\n",
        "    dtype=torch.qint8            # use int8 for weights\n",
        ")\n",
        "\n",
        "quantized_model.eval()          # evaluation mode\n",
        "quantized_model.to(\"cpu\")       # make sure it's on CPU\n",
        "\n",
        "quantized_acc = evaluate(quantized_model, test_loader, device=\"cpu\")\n",
        "print(f\"Test Accuracy (after quantizing linear layers only): {quantized_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CFD6jRfjbS4W",
      "metadata": {
        "id": "CFD6jRfjbS4W"
      },
      "source": [
        "Step 9: Measure Inference Latency-\n",
        "We measure and compare inference time (latency) before and after quantization. This step helps show the speed improvement that quantization can provide, especially on CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "P4sOC6XbWIBe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4sOC6XbWIBe",
        "outputId": "94ced7cc-773e-4d7c-cb03-e93ae0797779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference latency before quantization (GPU): 10.45 ms\n",
            "Inference latency after quantization (CPU): 2142.26 ms\n"
          ]
        }
      ],
      "source": [
        "# Step 9: Inference Latency Comparison\n",
        "def measure_latency(model, loader, device='cpu', n_samples=10):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    times = []\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for images, _ in loader:\n",
        "            images = images.to(device)\n",
        "            start = time.time()\n",
        "            _ = model(images)\n",
        "            end = time.time()\n",
        "            times.append((end - start) * 1000)  # milliseconds\n",
        "            count += 1\n",
        "            if count >= n_samples:\n",
        "                break\n",
        "    return np.mean(times)\n",
        "\n",
        "# Measure latency\n",
        "latency_before = measure_latency(model, test_loader, device='cuda')  # original model on GPU\n",
        "latency_after = measure_latency(quantized_model, test_loader, device='cpu')  # quantized model on CPU\n",
        "\n",
        "print(f\"Inference latency before quantization (GPU): {latency_before:.2f} ms\")\n",
        "print(f\"Inference latency after quantization (CPU): {latency_after:.2f} ms\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5lUomDsHbemi",
      "metadata": {
        "id": "5lUomDsHbemi"
      },
      "source": [
        "Step 10: Report Summary Metrics-\n",
        "We calculate and print:\n",
        "\n",
        "Memory saving (%) after quantization\n",
        "\n",
        "Accuracy drop caused by quantization\n",
        "\n",
        "Test accuracies before and after fine-tuning and quantization\n",
        "\n",
        "Inference latency (in milliseconds) before and after quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "FXTbS1TyXg4K",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXTbS1TyXg4K",
        "outputId": "e712acbe-c4c6-4dbd-8e34-706764fb21fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Model Size: 16.85 MB\n",
            "Quantized Model Size: 16.45 MB\n",
            "Memory Saving after Quantization: 2.33%\n",
            "Accuracy Drop after Quantization: -0.40%\n"
          ]
        }
      ],
      "source": [
        "# Step 10: Inference Latency Comparison\n",
        "\n",
        "def get_model_size(model, filename=\"temp.pth\"):\n",
        "    torch.save(model.state_dict(), filename)\n",
        "    size_mb = os.path.getsize(filename) / 1e6  # MB\n",
        "    os.remove(filename)\n",
        "    return size_mb\n",
        "\n",
        "# Measure size of both models\n",
        "original_size = get_model_size(model, \"original.pth\")\n",
        "quantized_size = get_model_size(quantized_model, \"quantized.pth\")\n",
        "\n",
        "# Accuracy metrics (already evaluated earlier)\n",
        "# finetuned_acc: accuracy after fine-tuning\n",
        "# quantized_acc: accuracy after quantization\n",
        "\n",
        "# Memory savings and accuracy drop\n",
        "memory_saved = ((original_size - quantized_size) / original_size) * 100\n",
        "accuracy_drop = finetuned_acc - quantized_acc\n",
        "\n",
        "print(f\"Original Model Size: {original_size:.2f} MB\")\n",
        "print(f\"Quantized Model Size: {quantized_size:.2f} MB\")\n",
        "print(f\"Memory Saving after Quantization: {memory_saved:.2f}%\")\n",
        "print(f\"Accuracy Drop after Quantization: {accuracy_drop:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Step 11:Latency measurement function"
      ],
      "metadata": {
        "id": "Q6kVVm5mJZkD"
      },
      "id": "Q6kVVm5mJZkD"
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11:Latency measurement function\n",
        "def measure_latency(model, device, dataset, num_samples=10):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    times = []\n",
        "    indices = random.sample(range(len(dataset)), num_samples)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx in indices:\n",
        "            img, _ = dataset[idx]\n",
        "            img = img.unsqueeze(0).to(device)  # Add batch dimension\n",
        "            start = time.time()\n",
        "            _ = model(img)\n",
        "            end = time.time()\n",
        "            times.append((end - start) * 1000)  # Convert to ms\n",
        "\n",
        "    return sum(times) / len(times)\n",
        "\n",
        "fp32_latency = measure_latency(model_fp32, device, food_test)\n",
        "int8_latency = measure_latency(model_int8, 'cpu', food_test)\n",
        "\n",
        "# Print results\n",
        "print(f\"Average Inference Latency (FP32) : {fp32_latency:.2f} ms\")\n",
        "print(f\"Average Inference Latency (INT8) : {int8_latency:.2f} ms\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMTJGOv_yVbD",
        "outputId": "b1fb1bee-778a-463c-b790-aec80d2be8f8"
      },
      "id": "BMTJGOv_yVbD",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Inference Latency (FP32) : 8.84 ms\n",
            "Average Inference Latency (INT8) : 39.43 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary of Metrics\n",
        "\n",
        "print(\"Summary of Model Evaluation Metrics\")\n",
        "\n",
        "print(\"Using device: cuda\")\n",
        "print(\"Test Accuracy (before fine-tuning): 0.80%\")\n",
        "print(\"Test Accuracy (after fine-tuning): 20.70%\")\n",
        "print(\"Model size before quantization: 16.85 MB\")\n",
        "print(\"Model size after quantization: 16.46 MB\")\n",
        "print(\"Test Accuracy (after quantizing linear layers only): 21.10%\")\n",
        "print(\"Inference latency before quantization (GPU): 10.45 ms\")\n",
        "print(\"Inference latency after quantization (CPU): 2142.26 ms\")\n",
        "print(\"Original Model Size: 16.85 MB\")\n",
        "print(\"Quantized Model Size: 16.45 MB\")\n",
        "print(\"Memory Saving after Quantization: 2.33%\")\n",
        "print(\"Accuracy Drop after Quantization: -0.40%\")\n",
        "print(\"Average Inference Latency (FP32) : 8.84 ms\")\n",
        "print(\"Average Inference Latency (INT8) : 39.43 ms\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsaRZtfmEY8U",
        "outputId": "7a4b18bb-ab4b-43f9-febe-1353cdb7bd27"
      },
      "id": "HsaRZtfmEY8U",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary of Model Evaluation Metrics\n",
            "Using device: cuda\n",
            "Test Accuracy (before fine-tuning): 0.80%\n",
            "Test Accuracy (after fine-tuning): 20.70%\n",
            "Model size before quantization: 16.85 MB\n",
            "Model size after quantization: 16.46 MB\n",
            "Test Accuracy (after quantizing linear layers only): 21.10%\n",
            "Inference latency before quantization (GPU): 10.45 ms\n",
            "Inference latency after quantization (CPU): 2142.26 ms\n",
            "Original Model Size: 16.85 MB\n",
            "Quantized Model Size: 16.45 MB\n",
            "Memory Saving after Quantization: 2.33%\n",
            "Accuracy Drop after Quantization: -0.40%\n",
            "Average Inference Latency (FP32) : 8.84 ms\n",
            "Average Inference Latency (INT8) : 39.43 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation:\n",
        "The results from the Food101 model analysis reveal several key insights:\n",
        "\n",
        "Pre-trained Model Performance:\n",
        "The original EfficientNet-B0 model, without any fine-tuning, achieved a very low accuracy of 0.80% on the Food101 dataset. This indicates that pre-trained models, which are typically trained on general datasets like ImageNet, may not directly generalize well to specialized tasks such as food classification.\n",
        "\n",
        "Impact of Fine-tuning:\n",
        "After fine-tuning the model on the Food101 dataset, the accuracy improved significantly to 20.70%. Although this accuracy is still modest, it clearly demonstrates that fine-tuning enables the model to adapt to domain-specific features, substantially enhancing performance.\n",
        "\n",
        "Quantization Effects:\n",
        "Quantizing only the linear layers of the model resulted in a slight increase in accuracy to 21.10%, indicating that quantization did not degrade model performance. Additionally, the model size was reduced by approximately 2.33%, which is beneficial for deploying models in resource-constrained environments such as mobile devices.\n",
        "\n",
        "Inference Latency:\n",
        "The original FP32 model running on a GPU exhibited low inference latency of around 10 ms, whereas the quantized INT8 model running on a CPU had much higher latency, approximately 2142 ms. The average measured inference latency was 8.84 ms for the FP32 model and 39.43 ms for the INT8 model on the tested devices. This shows that while quantization reduces model size and can maintain accuracy, improvements in latency heavily depend on the hardware being used. Quantization alone does not guarantee faster inference unless supported by compatible hardware acceleration.\n",
        "\n",
        "Summary:\n",
        "These findings highlight the importance of domain-specific fine-tuning to achieve reasonable accuracy in specialized applications like food classification. Quantization offers advantages in reducing model size without sacrificing accuracy, but its impact on inference speed varies depending on the deployment hardware. For practical deployment, both model optimization and hardware compatibility must be considered."
      ],
      "metadata": {
        "id": "TLupMkc0DvLM"
      },
      "id": "TLupMkc0DvLM"
    },
    {
      "cell_type": "markdown",
      "id": "DfGASp4Yds-V",
      "metadata": {
        "id": "DfGASp4Yds-V"
      },
      "source": [
        "Conclusion-\n",
        "The fine-tuned EfficientNet-B0 model demonstrates a substantial improvement in classification accuracy on the Food101 dataset, increasing from a low baseline of 0.80% to 20.70% after training. This underscores the importance of adapting pre-trained models to domain-specific datasets for meaningful performance gains. Applying quantization reduced the model size by approximately 2.33%, with minimal impact on accuracy, confirming that quantization is an effective technique to compress models for deployment without significant performance loss. However, the inference latency after quantization increased substantially on the CPU, highlighting that hardware capabilities greatly influence the practical benefits of quantization. Overall, fine-tuning combined with model quantization offers an effective strategy to balance accuracy, model size, and efficiency for real-world food classification applications.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}