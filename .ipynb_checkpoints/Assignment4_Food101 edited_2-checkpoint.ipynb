{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Fine-Tuning and Quantization of EfficientNet for Food Type Detection Using Food-101 Dataset"
      ],
      "metadata": {
        "id": "DLGNq1-m35KV"
      },
      "id": "DLGNq1-m35KV"
    },
    {
      "cell_type": "code",
      "source": [
        "import os, tempfile, torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.quantization import quantize_dynamic\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# MODEL --------------------------------------------------------------------\n",
        "# Download pre-trained model\n",
        "model = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.DEFAULT)\n",
        "\n",
        "#model.classifier[1] = nn.Linear(model.last_channel, 10)\n",
        "\n",
        "# Modify input layer to accept 1 channel instead of 3\n",
        "model.features[0][0] = nn.Conv2d(\n",
        "    in_channels=1,\n",
        "    out_channels=model.features[0][0].out_channels,\n",
        "    kernel_size=model.features[0][0].kernel_size,\n",
        "    stride=model.features[0][0].stride,\n",
        "    padding=model.features[0][0].padding,\n",
        "    bias=False\n",
        ")\n",
        "\n",
        "# Modify output layer to output 10 classes\n",
        "in_features = model.classifier[3].in_features\n",
        "model.classifier[3] = nn.Linear(in_features, 10)\n",
        "\n",
        "model.to(device)\n",
        "torch.save(model.state_dict(), \"pre-trainedmodel-without-finetuning.pth\")\n",
        "\n",
        "# DATA for Fashion ---------------------------------------------------------------------\n",
        "'''\n",
        "trf = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.2860,), (0.3530,))\n",
        "])\n",
        "\n",
        "\n",
        "train_set = datasets.FashionMNIST(root=\".\", train=True, download=True, transform=trf)\n",
        "test_set  = datasets.FashionMNIST(root=\".\", train=False, download=True, transform=trf)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True,  num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_set,  batch_size=256,shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "'''\n",
        "\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),       # MobileNetV3 expects 224x224\n",
        "    transforms.ToTensor(),        # Keeps 1 channel\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize 1 channel\n",
        "])\n",
        "\n",
        "\n",
        "train_dataset = FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "# EVALUATION ---------------------------------------------------------------\n",
        "@torch.no_grad()\n",
        "def accuracy(net):\n",
        "    net.eval()\n",
        "    hits = total = 0\n",
        "    for X, y in test_loader:\n",
        "        X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "        hits  += (net(X).argmax(1) == y).sum().item()\n",
        "        total += y.size(0)\n",
        "    return hits / total * 100\n",
        "\n",
        "# measuring accuracy of pretrained model without fine tuning on test dataset for chosen application of cloth type detector\n",
        "fp32_acc_no_fine = accuracy(model)\n",
        "print(f\"Test Accuracy of pre-trained model without any fine tuning: {fp32_acc_no_fine:5.2f}%\")\n",
        "\n",
        "# FINETUNing the pre-trained model with selected training dataset for given application-----------------------------------------------------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "EPOCHS = 1\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for X, y in train_loader:\n",
        "        X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(X), y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | loss {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "torch.save(model.state_dict(), \"pre-trained-model-with-finetuning.pth\")\n",
        "\n",
        "\n",
        "fp32_acc = accuracy(model)\n",
        "print(f\"Test accuracy of pre-trained model after fine-tuning and before quantization : {fp32_acc:5.2f}%\")\n",
        "\n",
        "def model_size_mb(net, fname):\n",
        "    torch.save(net.state_dict(), fname)\n",
        "    return os.path.getsize(fname) / 1_000_000\n",
        "\n",
        "tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".pt\")\n",
        "fp32_size = model_size_mb(model, tmp.name)\n",
        "print(f\"Model size before qunatization: {fp32_size:5.2f} MB\")\n",
        "tmp.close()\n",
        "\n",
        "# QUANTISATION -------------------------------------------------------------\n",
        "quantised = quantize_dynamic(\n",
        "    model.cpu(),\n",
        "    {nn.Linear, nn.Conv2d},\n",
        "    dtype=torch.qint8\n",
        ").to(device)\n",
        "quantised.eval()\n",
        "\n",
        "\n",
        "def accuracy1(net):\n",
        "    net.eval()\n",
        "    hits, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in test_loader:\n",
        "            X, y = X.to('cpu'), y.to('cpu')\n",
        "            hits += (net(X).argmax(1) == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    return hits / total * 100\n",
        "\n",
        "torch.save(model.state_dict(), \"quantized-pre-trainedmodel-with-finetuning.pth\")\n",
        "\n",
        "int8_acc = accuracy1(quantised.to('cpu'))\n",
        "print(f\"Test accuracy of pre-trained model after fine-tuning and after quantization : {int8_acc:5.2f}%\")\n",
        "\n",
        "tmp_q = tempfile.NamedTemporaryFile(delete=False, suffix=\".pt\")\n",
        "int8_size = model_size_mb(quantised.cpu(), tmp_q.name)\n",
        "print(f\"Model size after INT8 qunatization: {int8_size:5.2f} MB\")\n",
        "tmp_q.close()\n",
        "\n",
        "print(f\"\\nMemory saving : {(1 - int8_size / fp32_size) * 100:4.1f}%\")\n",
        "print(f\"Accuracy drop : {fp32_acc - int8_acc:4.2f} percentage points\")\n"
      ],
      "metadata": {
        "id": "80y0K0UUD14q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cdb0bcf-03bb-416b-d005-9c0d31cdc3f6"
      },
      "id": "80y0K0UUD14q",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_large-5c1a4163.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-5c1a4163.pth\n",
            "100%|██████████| 21.1M/21.1M [00:00<00:00, 81.5MB/s]\n",
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 11.6MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 167kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.08MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 13.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy of pre-trained model without any fine tuning: 10.25%\n",
            "Epoch 1/1 | loss 0.3590\n",
            "Test accuracy of pre-trained model after fine-tuning and before quantization : 90.17%\n",
            "Model size before qunatization: 17.06 MB\n",
            "Test accuracy of pre-trained model after fine-tuning and after quantization : 90.16%\n",
            "Model size after INT8 qunatization: 13.34 MB\n",
            "\n",
            "Memory saving : 21.8%\n",
            "Accuracy drop : 0.01 percentage points\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a5e591e-a38c-4577-9d82-2123c2bd77a6",
      "metadata": {
        "id": "9a5e591e-a38c-4577-9d82-2123c2bd77a6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "601f1d82",
      "metadata": {
        "id": "601f1d82"
      },
      "source": [
        "## Food Type Detection Using EfficientNet_B0 on Food101"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Load Pretrained Model-\n",
        "We load the EfficientNet model (e.g., efficientnet_b0) which has been pre-trained on ImageNet. This helps the model start with strong visual features without training from scratch."
      ],
      "metadata": {
        "id": "XLEN5z4zaNvd"
      },
      "id": "XLEN5z4zaNvd"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "b117c0a4",
      "metadata": {
        "id": "b117c0a4",
        "outputId": "1a2dc199-8449-4165-899e-d3a35f300411",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Step 1: Import Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import Food101\n",
        "from torchvision.models import efficientnet_b0\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Load and Prepare Dataset-\n",
        "We use the Food101 dataset, a large vision dataset of 101 food categories. Images are transformed (resized, normalized, converted to tensor) and split into training and test loaders for training and evaluation."
      ],
      "metadata": {
        "id": "6E19A-yxaiy1"
      },
      "id": "6E19A-yxaiy1"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "b81748ab",
      "metadata": {
        "id": "b81748ab"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 2: Load and Transform Food101 Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "food_train = Food101(root='./data', split='train', transform=transform, download=True)\n",
        "food_test = Food101(root='./data', split='test', transform=transform, download=True)\n",
        "\n",
        "# Subsample for faster training/testing\n",
        "train_subset, _ = random_split(food_train, [5000, len(food_train) - 5000])\n",
        "test_subset, _ = random_split(food_test, [1000, len(food_test) - 1000])\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_subset, batch_size=32, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Replace the Classifier-\n",
        "We replace the final classification layer of the pretrained EfficientNet model to match our dataset (101 classes for Food101 instead of 1000 )."
      ],
      "metadata": {
        "id": "5WeM9QXXaq-I"
      },
      "id": "5WeM9QXXaq-I"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "bf2e076e",
      "metadata": {
        "id": "bf2e076e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 3: Load Pretrained EfficientNet_B0 and Modify Output Layer\n",
        "model = efficientnet_b0(pretrained=True)\n",
        "model.classifier[1] = nn.Linear(model.classifier[1].in_features, 101)\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Evaluate Pretrained Model (Before Fine-Tuning)\n",
        "We evaluate the model on the test set before any fine-tuning. This gives us a baseline accuracy using the pretrained features."
      ],
      "metadata": {
        "id": "F-rnYKWlawTx"
      },
      "id": "F-rnYKWlawTx"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "df6d7e8b",
      "metadata": {
        "id": "df6d7e8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b061d22f-5294-461c-db36-64b1501dae32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy (before fine-tuning): 1.50%\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Test Accuracy Before Fine-Tuning\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "baseline_acc = evaluate(model, test_loader, device)\n",
        "print(f\"Test Accuracy (before fine-tuning): {baseline_acc:.2f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Fine-Tune the Model-\n",
        "We train the model for one epoch on the Food101 training set. This helps the model adapt its learned features to the specific task of food classification."
      ],
      "metadata": {
        "id": "NgjpRGhia28r"
      },
      "id": "NgjpRGhia28r"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "e4c45ce1",
      "metadata": {
        "id": "e4c45ce1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e75a1beb-8d00-4014-8127-d2fc0342171d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy (after fine-tuning): 24.60%\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Fine-Tune for 1 Epoch\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "model.train()\n",
        "for images, labels in train_loader:\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "finetuned_acc = evaluate(model, test_loader, device)\n",
        "print(f\"Test Accuracy (after fine-tuning): {finetuned_acc:.2f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Evaluate Model (After Fine-Tuning)-\n",
        "After fine-tuning, we evaluate the model again on the test set to see how much performance improved compared to the pretrained baseline."
      ],
      "metadata": {
        "id": "Xxpa02X-a8ep"
      },
      "id": "Xxpa02X-a8ep"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "fdfade53",
      "metadata": {
        "id": "fdfade53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6551a053-9569-496d-d6b9-cd174b199887"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size before quantization: 16.85 MB\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Step 6: Save Model and Measure Size\n",
        "torch.save(model.state_dict(), \"efficientnet_finetuned.pth\")\n",
        "original_size = os.path.getsize(\"efficientnet_finetuned.pth\") / 1e6  # MB\n",
        "print(f\"Model size before quantization: {original_size:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Measure Model Size Before Quantization-\n",
        "We save the model to disk and measure its file size. This gives the size of the full-precision (FP32) model before applying any compression."
      ],
      "metadata": {
        "id": "rOmgE2gbbCQG"
      },
      "id": "rOmgE2gbbCQG"
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Quantize Model\n",
        "quantized_model = torch.quantization.quantize_dynamic(copy.deepcopy(model), {nn.Linear}, dtype=torch.qint8)\n",
        "torch.save(quantized_model.state_dict(), \"efficientnet_quantized.pth\")\n",
        "quantized_size = os.path.getsize(\"efficientnet_quantized.pth\") / 1e6  # MB\n",
        "print(f\"Model size after quantization: {quantized_size:.2f} MB\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdrK4Sl_QgXp",
        "outputId": "99fb1b46-d59b-43c4-b1cf-ef05c3ee3f5b"
      },
      "id": "SdrK4Sl_QgXp",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size after quantization: 16.46 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8: Apply Dynamic Quantization-\n",
        "We use dynamic quantization to convert Linear layers of the model to 8-bit integers (INT8), which reduces model size and may speed up inference — especially on CPU."
      ],
      "metadata": {
        "id": "VzCr354AbIAB"
      },
      "id": "VzCr354AbIAB"
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Apply Dynamic Quantization\n",
        "from torch.quantization import quantize_dynamic\n",
        "\n",
        "quantized_model = quantize_dynamic(\n",
        "    model.cpu(),                 # move model to CPU before quantization\n",
        "    {torch.nn.Linear},           # specify layers to quantize\n",
        "    dtype=torch.qint8            # use int8 for weights\n",
        ")\n",
        "\n",
        "quantized_model.eval()          # evaluation mode\n",
        "quantized_model.to(\"cpu\")       # make sure it's on CPU\n",
        "\n",
        "quantized_acc = evaluate(quantized_model, test_loader, device=\"cpu\")\n",
        "print(f\"Test Accuracy (after quantizing linear layers only): {quantized_acc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6aHZq5FVsde",
        "outputId": "6cce0b8f-072a-4b58-ab5d-73c6c4139260"
      },
      "id": "w6aHZq5FVsde",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy (after quantizing linear layers only): 24.80%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 9: Measure Inference Latency-\n",
        "We measure and compare inference time (latency) before and after quantization. This step helps show the speed improvement that quantization can provide, especially on CPU."
      ],
      "metadata": {
        "id": "CFD6jRfjbS4W"
      },
      "id": "CFD6jRfjbS4W"
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Measure Inference Latency\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# Step 9: Inference Latency Comparison\n",
        "def measure_latency(model, loader, device='cpu', n_samples=10):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    times = []\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for images, _ in loader:\n",
        "            images = images.to(device)\n",
        "            start = time.time()\n",
        "            _ = model(images)\n",
        "            end = time.time()\n",
        "            times.append((end - start) * 1000)  # milliseconds\n",
        "            count += 1\n",
        "            if count >= n_samples:\n",
        "                break\n",
        "    return np.mean(times)\n",
        "\n",
        "# Measure latency\n",
        "latency_before = measure_latency(model, test_loader, device='cuda')  # original model on GPU\n",
        "latency_after = measure_latency(quantized_model, test_loader, device='cpu')  # quantized model on CPU\n",
        "\n",
        "print(f\"Inference latency before quantization (GPU): {latency_before:.2f} ms\")\n",
        "print(f\"Inference latency after quantization (CPU): {latency_after:.2f} ms\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4sOC6XbWIBe",
        "outputId": "4fd233d5-01f5-4e7d-c762-b73eac5cdd95"
      },
      "id": "P4sOC6XbWIBe",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference latency before quantization (GPU): 14.47 ms\n",
            "Inference latency after quantization (CPU): 2020.89 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 10: Report Summary Metrics-\n",
        "We calculate and print:\n",
        "\n",
        "Memory saving (%) after quantization\n",
        "\n",
        "Accuracy drop caused by quantization\n",
        "\n",
        "Test accuracies before and after fine-tuning and quantization\n",
        "\n",
        "Inference latency (in milliseconds) before and after quantization"
      ],
      "metadata": {
        "id": "5lUomDsHbemi"
      },
      "id": "5lUomDsHbemi"
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Report Summary Metrics We calculate and print:\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "def measure_latency(model, loader, device, n_samples=10):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    times = []\n",
        "    images_shown = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, _ in loader:\n",
        "            images = images.to(device)\n",
        "            start = time.time()\n",
        "            _ = model(images)\n",
        "            end = time.time()\n",
        "            times.append((end - start) * 1000)  # convert to ms\n",
        "            images_shown += 1\n",
        "            if images_shown >= n_samples:\n",
        "                break\n",
        "\n",
        "    return np.mean(times)\n",
        "\n",
        "# Run latency for original (GPU) and quantized (CPU) models\n",
        "latency_before = measure_latency(model, test_loader, device='cuda')        # Original model on GPU\n",
        "latency_after = measure_latency(quantized_model, test_loader, device='cpu')  # Quantized model on CPU\n",
        "\n",
        "print(f\"Inference latency before quantization (GPU): {latency_before:.2f} ms\")\n",
        "print(f\"Inference latency after quantization (CPU): {latency_after:.2f} ms\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbEgXbm3XAyw",
        "outputId": "89ad6348-13f5-4410-b58e-136bc041745a"
      },
      "id": "YbEgXbm3XAyw",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference latency before quantization (GPU): 10.12 ms\n",
            "Inference latency after quantization (CPU): 1960.31 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "def get_model_size(model, filename=\"temp.pth\"):\n",
        "    torch.save(model.state_dict(), filename)\n",
        "    size_mb = os.path.getsize(filename) / 1e6  # MB\n",
        "    os.remove(filename)\n",
        "    return size_mb\n",
        "\n",
        "# Measure size of both models\n",
        "original_size = get_model_size(model, \"original.pth\")\n",
        "quantized_size = get_model_size(quantized_model, \"quantized.pth\")\n",
        "\n",
        "# Accuracy metrics (already evaluated earlier)\n",
        "# finetuned_acc: accuracy after fine-tuning\n",
        "# quantized_acc: accuracy after quantization\n",
        "\n",
        "# Memory savings and accuracy drop\n",
        "memory_saved = ((original_size - quantized_size) / original_size) * 100\n",
        "accuracy_drop = finetuned_acc - quantized_acc\n",
        "\n",
        "print(f\"Original Model Size: {original_size:.2f} MB\")\n",
        "print(f\"Quantized Model Size: {quantized_size:.2f} MB\")\n",
        "print(f\"Memory Saving after Quantization: {memory_saved:.2f}%\")\n",
        "print(f\"Accuracy Drop after Quantization: {accuracy_drop:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXTbS1TyXg4K",
        "outputId": "bef551ca-be22-46a2-ec88-d4fca00a2ee8"
      },
      "id": "FXTbS1TyXg4K",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Model Size: 16.85 MB\n",
            "Quantized Model Size: 16.45 MB\n",
            "Memory Saving after Quantization: 2.33%\n",
            "Accuracy Drop after Quantization: -0.20%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Summary\n",
        "| Metric                                 | Value                        |\n",
        "| -------------------------------------- | ---------------------------- |\n",
        "| **Device Used**                        | CUDA (T4 GPU)                |\n",
        "| **Test Accuracy (Before Fine-Tuning)** | 1.50%                        |\n",
        "| **Test Accuracy (After Fine-Tuning)**  | 24.60%                       |\n",
        "| **Model Size (Before Quantization)**   | 16.85 MB                     |\n",
        "| **Model Size (After Quantization)**    | 16.46 MB                     |\n",
        "| **Memory Saved After Quantization**    | 2.33%                        |\n",
        "| **Test Accuracy (After Quantization)** | 24.80%                       |\n",
        "| **Accuracy Drop After Quantization**   | **-0.20%** (slight increase) |\n",
        "| **Inference Latency (Before, GPU)**    | 10.12 ms                     |\n",
        "| **Inference Latency (After, CPU)**     | 1960.31 ms                   |\n"
      ],
      "metadata": {
        "id": "yJx6OvUAcA-5"
      },
      "id": "yJx6OvUAcA-5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion\n",
        "The EfficientNet model was successfully fine-tuned on the Food101 dataset, achieving a 24.60% test accuracy after one epoch of fine-tuning. While this accuracy is modest, it demonstrates that the model is starting to learn useful features specific to the food classification task.\n",
        "\n",
        "After applying dynamic quantization to linear layers, the model size was reduced from 16.85 MB to 16.46 MB, yielding a 2.33% memory saving. This is a minor reduction since only the linear layers were quantized.\n",
        "\n",
        "Interestingly, quantization led to a slight increase in accuracy (+0.20%), possibly due to regularization effects.\n",
        "\n",
        "However, inference latency significantly increased from ~10 ms (GPU) to ~1960 ms (CPU) post-quantization. This is expected because dynamic quantization is designed to optimize CPU inference, and your original model was running on GPU (which is significantly faster for this model type).\n",
        "\n",
        "This highlights an important trade-off: while quantization can save memory and enable model deployment on lower-end hardware (e.g., mobile devices), it may lead to slower inference unless further optimized for CPU or edge environments.\n",
        "\n"
      ],
      "metadata": {
        "id": "DfGASp4Yds-V"
      },
      "id": "DfGASp4Yds-V"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}